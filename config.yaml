# config.yaml
train:
  num_epochs: 5000
  text_learning_rate: 0.001
  molecule_learning_rate: 0.001
  logit_scale_learning_rate: 0.001
  min_lr: 0.0001
  save_frequency: 1
  delete_previous_checkpoint: 1
  save_most_recent: 1
  save_best: 1
  

model:
  MolCLR:
    pretrained_folder: "MolCLR/ckpt/pretrained_gin/checkpoints"
    pretrained_model: "model.pth"
    num_layer: 5
    emb_dim: 300
    feat_dim: 512
    drop_ratio: 0
    pool: "mean"
    pred_n_layer: 2
    pred_act: "relu"
    out_dim: 512
    num_atom_type: 119
    num_chirality_tag: 3
    save_folder: "MolCLR/ckpt/pretrained_gin/checkpoints" 
  text:
    tokenizer: "dmis-lab/biobert-base-cased-v1.1"
    pretrained_model: "dmis-lab/biobert-base-cased-v1.1"
    additional_hidden_size: 512
    out_features: 512
 

wandb_project_name: "cldp2"


data:
  data_csv_path: "merged_description_smiles.csv"
  batch_size:  640 # 640, 448 is not working 
  shuffle: true

resume: "latest"
debug: 0
wandb: 1

skip_scheduler: 0
lr_scheduler: "cosine"
warmup: 782 #782

remote: 0
# resume: "latest"
logs: "logs"
name: "CLDP"
save_logs: 1
seed: 42