{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4654, grad_fn=<NllLossBackward>)\n",
      "here\n",
      "here3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here2\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from dataset import DrugDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "from MolCLR.dataset.dataset import MoleculeDatasetWrapper\n",
    "from MolCLR.models.ginet_finetune import GINet\n",
    "\n",
    "class ContrastiveLearningWithBioBERT(nn.Module):\n",
    "    def __init__(self, molecule_model):\n",
    "        super(ContrastiveLearningWithBioBERT, self).__init__()\n",
    "        \n",
    "        # Load the tokenizer and model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "        self.text_model = BertModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "        # Molecule model\n",
    "        self.molecule_model = molecule_model\n",
    "        # Freeze the molecule model\n",
    "        for param in self.molecule_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text, molecule_representation):\n",
    "        # Tokenize the input text\n",
    "        encoded_input = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Get the representation from the text model (bioBERT)\n",
    "        text_features = self.text_model(**encoded_input).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Calculate the cosine similarity between text and molecule representations\n",
    "        similarities = F.cosine_similarity(text_features.unsqueeze(0), molecule_representation.unsqueeze(1), dim=2)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarities, torch.zeros(similarities.shape[0], dtype=torch.long).to(similarities.device))\n",
    "        \n",
    "        # normalized features\n",
    "        # molecule_representation = molecule_representation / molecule_representation.norm(dim=1, keepdim=True)\n",
    "        # text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        # logit_scale = self.logit_scale.exp()\n",
    "        # logits_per_image = logit_scale * molecule_representation @ text_features.t()\n",
    "        # logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        # return logits_per_image, logits_per_text\n",
    "        return loss\n",
    "\n",
    "# Placeholder for the molecule model (for demonstration purposes)\n",
    "class MoleculeModelMock(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(MoleculeModelMock, self).__init__()\n",
    "        self.encoder = nn.Linear(100, embedding_dim) # Just a mock encoder\n",
    "        \n",
    "    def forward(self, molecule):\n",
    "        return self.encoder(molecule)\n",
    "\n",
    "# Initialize models\n",
    "molecule_model = MoleculeModelMock(embedding_dim=768) # Using 768 to match bioBERT's output dimension\n",
    "model_with_biobert = ContrastiveLearningWithBioBERT(molecule_model)\n",
    "\n",
    "# Test with random data\n",
    "texts = [\"Metformin is a first-line oral hypoglycemic agent.\", \"Aspirin is an analgesic.\"] * 16\n",
    "molecule_representations = torch.randn(32, 768) # Corresponding molecules for the batch\n",
    "\n",
    "loss = model_with_biobert(texts, molecule_representations)\n",
    "print(loss)\n",
    "\n",
    "# Training setup\n",
    "print(\"here\")\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "smiles_data = pd.read_csv(\"/home/luli/MolCLR/output_smiles.csv\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "dataset = MoleculeDatasetWrapper(6,4,0.1, \"/home/luli/MolCLR/output_smiles.csv\")\n",
    "molecule_model = GINet( \"classification\" ).to(\"cuda:0\")\n",
    "checkpoints_folder = os.path.join('MolCLR/ckpt', \"pretrained_gin\", 'checkpoints')\n",
    "state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'), map_location= \"cuda:0\")\n",
    "molecule_model.load_my_state_dict(state_dict)\n",
    "print(\"here3\")\n",
    "# Assuming you have already defined the molecule model and loaded the pretrained weights\n",
    "model = ContrastiveLearningWithBioBERT(molecule_model)\n",
    "optimizer = optim.Adam(model.text_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Test the DrugDataset\n",
    "# For demonstration purposes, I'm using placeholder paths. Replace with your actual paths.\n",
    "description_csv_path = \"/home/luli/drugBank/drugbank.csv\"\n",
    "molecule_csv_path = \"/home/luli/drugBank/output_smiles.csv\"\n",
    "dataset = DrugDataset(description_csv_path, molecule_csv_path)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "print(\"here2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_texts, batch_molecules = dataloader.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_texts = [batch_texts, batch_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "text_model = BertModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "encoded_input = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "       \n",
    "        # Get the representation from the text model (bioBERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = text_model(**encoded_input).last_hidden_state.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1142,  0.1500, -0.0950,  ...,  0.2557,  0.1438,  0.0029],\n",
       "        [ 0.1142,  0.1500, -0.0950,  ...,  0.2557,  0.1438,  0.0029]],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20164/199113599.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch_molecules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "batch_molecules.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19123/642809765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_molecules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "similarities = F.cosine_similarity(text_features.unsqueeze(0), batch_molecules.unsqueeze(1), dim=2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Compute the loss\n",
    "loss = F.cross_entropy(similarities, torch.zeros(similarities.shape[0], dtype=torch.long).to(similarities.device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
